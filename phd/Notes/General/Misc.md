## ðŸ” Softmax Function
The softmax function converts a vector of logits (real values) into a probability distribution:
$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=0}^{C-1} e^{x_j}}
$$
This ensures:
$$
\sum_i \text{softmax}(x_i) = 1
$$
Which produces a probability vector:
$$
\mathbf{p} = [p_0, p_1, \dots, p_{C-1}]
$$

## ðŸ§® Logits â€” Notes

### â“Â _What is a logit?_

- AÂ **logit**Â is the raw output of a model before applying a probability transformation likeÂ **softmax**Â (for multi-class) orÂ **sigmoid**Â (for binary).
- Logits areÂ **real-valued**Â andÂ **unbounded**Â â€” they are not probabilities.
- They represent a modelâ€™sÂ **relative confidence**, not a calibrated score.

---

### â“Â _Do logits need to sum to 1?_

- **No.**Â Logits do not and should not sum to 1.
- It is theÂ **softmax**Â transformation that converts logits into a probability distribution summing to 1.

---

### â“Â _How do I turn logits into meaningful scores?_

- ApplyÂ **softmax**Â to get probabilities.
- UseÂ **log-softmax**Â if you're interested in log-probabilities (e.g. for loss computation).
- To compare or rank logits without computing probabilities, you can use:
    - RelativeÂ **margins**:Â `max - logits[true_class]`
    - **Negative log-softmax**: gives a calibrated penalty for low confidence.

---

## ðŸ“‰ Cross-Entropy Loss â€” Notes

You have:
- A vector ofÂ **logits**$x=[x_0,x1_,â€¦,x,Câˆ’1]$(raw model outputs, one per class)
- AÂ **true class**Â label$yâˆˆ{0,1,â€¦,Câˆ’1}$
- The formula is
$$CE(x,y)=âˆ’x_y+logâ¡(âˆ‘_j e^{x_j})$$

---
### â“Â _Do I need probabilities for cross-entropy?_

- **No.**Â Cross-entropy expectsÂ **logits**, not softmax outputs.
- It internally uses theÂ **log-sum-exp trick**Â for numerical stability.

---
### â“Â _Does the true class logit really matter?_

- **Yes, critically.**
- The lossÂ **decreases**Â asÂ xyÂ increases relative to other logits.
- The model is optimized toÂ **maximize**Â the logit for the true class whileÂ **minimizing**Â others.

---
### â“Â _How do I achieve perfect (zero) loss?_
- Set$x_yâ†’âˆž$and all other$logitsÂ â†’âˆ’âˆž$
- This makesÂ softmax(x)y=1, so:$CE=âˆ’logâ¡(1)=0$

---

### ðŸ§® Cross-Entropy relation to Softmax

Starting from$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=0}^{C-1} e^{x_j}}$, if we use$-\log$on it we get
$$-\log(\text{softmax}(x_i)) = -\log(\frac{e^{x_i}}{\sum_{j=0}^{C-1} e^{x_j}}) = âˆ’x_y+logâ¡(âˆ‘_j e^{x_j}) = CE(x,y)$$

### âœ… Summary
- **Softmax**Â normalizes logits into probabilities.
- **Cross-entropy**Â computes the negative log-likelihood of the true class.
- **Direct logit-based CE**Â avoids computing probabilities explicitly.

# ðŸ§  Energy-Based Models (EBMs)

Energy-Based Models (EBMs) define a scalar energy function$E(x, y) \in \mathbb{R}$over inputâ€“output pairs. The model's goal is to assign **low energy to valid outputs** and **high energy to invalid ones**

---

## ðŸ”‹ Core Concepts

- **Energy Function**:$E(x, y)$measures how "bad" or "unpreferred" a candidate output$y$is for input$x$.
- **No direct prediction** of probabilities like in softmax or cross-entropy models.
- **Inference** is about finding $\arg\min_y E(x, y)$.

---

## ðŸ§® Training Objectives

### 1. **Contrastive Loss** (Margin-based):

$$
\mathcal{L} = \max\left(0, E(x, y^+) - E(x, y^-) + \text{margin} \right)
$$

 - $y^+$: valid (gold) output  
- $y^-$: corrupted (invalid) output

### 2. **Negative Log-Likelihood** (Optional):

Define probability from energy (Boltzmann Distribution):

$$
P(y|x) = \frac{e^{-E(x,y)}}{Z(x)}, \quad Z(x) = \sum_{y'} e^{-E(x, y')}
$$

Loss becomes:

$$
\mathcal{L} = E(x, y^+) + \log \sum_{y'} e^{-E(x, y')}
$$

> Computing$Z(x)$is usually intractable â†’ approximated with sampling.

---

## ðŸ’¡ Comparison: EBM vs Regular Models

|                         | Regular NN                     | EBM                                |
| ----------------------- | ------------------------------ | ---------------------------------- |
| **Output**              | Probabilities / logits         | Arbitrary structured object        |
| **Loss**                | Cross-entropy, MSE, etc.       | Energy: low for good, high for bad |
| **Structure awareness** | Rare (unless explicitly added) | Built-in via energy design         |
| **Ground truth needed** | Yes                            | Optional (can be unsupervised)     |

---

## ðŸ§© Use Case: Enforcing Loop Structure in a Graph

Given a predicted adjacency matrix$A \in [0, 1]^{N \times N}$:

### Loop Participation Score:

$$
S_{i,j} = \sum_{k=1}^{K} A[i,j] \cdot A^k[j,i]
$$

This soft score reflects how much arc$A[i,j]$participates in loops.

### Backward Jump Penalty:

Let$B = \{ (i,j) \mid i > j \}$be backward arcs.

Estimate if loops involving$A[i,j]$exceed 1 backward jump:

$$
\text{penalty}(A) = \lambda \cdot \max(0, \text{expected\_backward\_jumps} - 1)^2
$$

---

## ðŸ§ª Using Energy as a Regularizer

You can use the energy function as a **soft constraint** in a regular loss:

$$
\mathcal{L}_{TOTAL} = \mathcal{L}_{Logit}+\lambda E
$$
