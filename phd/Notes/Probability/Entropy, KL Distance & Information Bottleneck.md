# Entropy

Let's say we want to measure the surprise of an experiment, say the flip of an unfair coin where $p(T) = 0.9$ and $p(H) = 0.1$. A naive approach could be to simply set our surprise as the inverse of the probability $S = 1/p$, since it's inversely correlated to our expectation of an event

However, it would be nicer if $S$ of a sure event would be $0$, currently it would be $1$. To solve this we set the surprise to the log of the inverse $S = \log(1/p)$, which by log properties is $S = -\log(p)$

Now, the Entropy is simply the **average surprise**, or the expected surprise, which is intuitively calculated by multiplying the surprise of an event with its probability: $H = -\sum_x p(x)\log(p(x))$ 

As an example: our unfair coin had a surprise for heads of $-\log(0.9)$ = 0.152 and $-\log(0.1) = 3.322$ so $H = 0.9\cdot 0.152 + 3.322\cdot 1 = 0.484$. A fair coin would have $H = 0.5\cdot 1 + 0.5\cdot 1 = 1$, a higher entropy since its more "disordered"
# KL Distance

There are many ways in which distributions can differ, since they aren't single numbers or points but functions. As such we need a representative way of defining distance between two distributions: the KL distance does exactly that

Imagine having a coin with a fairness (AKA the prob of getting heads and tails) which we don't know, we have one hypothesis tho. We define the KL distance between their distributions as *how likely is one distribution able to recreate examples generated from the other?* Which we rigorously compute as follows:
1. Say the two fairnesses have probabilities $p_T,p_H$ and $q_T,q_H$ respectively
2. Flip the coin $N$ times, then we measure the probability that it was generated by each distribution. This is just multiplying each result by its probability. Say we got $H$ heads and $T$ tails$$
p_T^Tp_H^H\quad q_T^*q_H^H
$$
3. We want the ratio of these two $\frac{p_T^Tp_H^H}{q_T^Tq_H^H}$, we then raise it to $N$ and log it $\log(\frac{p_T^Tp_H^H}{q_T^Tq_H^H}^N)$
4. Applying some log properties we get $$T/N\log(p_T)+H/N\log(p_H)+T/N\log(q_T)+H/N\log(q_H)$$
5. If we consider infinite flips then $T/N \to p_T$ and $T/H \to p_H$ (since we consider this the "real" prob). So we get to (again with some log properties): $p_T\log(p_T/q_T)+p_H\log(p_H/q_H)$

Generalizing to $K$ discrete results we get $D_{KL}(P\|Q) = \sum_x p(x)\log(p(x)/q(x))$ 

