---

kanban-plugin: board

---

## üöÄ To Read

- [ ] ERNIE: Enhanced Language Representation with Informative Entities (Zhang et al., 2019)
- [ ] A Hierarchical Multi-Granularity Model for Joint Entity and Relation Extraction (Zheng et al., 2019)
- [ ] Boundary Detection and Aggregation for Named Entity Recognition (Tan et al., 2018)
- [ ] Unsupervised Named Entity Recognition with Denoising Language Models (Zhou et al., 2023)
- [ ] Discriminative Learning of Latent Events for Entity Extraction (Li et al., 2022)
- [ ] Few-NERD: A Few-shot Benchmark for Named Entity Recognition (Ding et al., 2021)
- [ ] Pretrained Language Models for Named Entity Recognition: A Survey (Li et al., 2021)
- [ ] NER with Large Language Models: A Survey (Chen et al., 2023)


## üìñ Reading


## ‚úçÔ∏è To Take Notes


## üìö Notes Taken

**Complete**


***

## Archive

- [ ] CycleNER: An Unsupervised Training Approach for Named Entity Recognition (Iovine et al., 2022)
- [ ] LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention (Yamada et al., 2020)
- [ ] Unified Named Entity Recognition as Word-Word Relation Classification (Li et al., 2021)
- [ ] SpanBERT: Improving Pre-training by Representing and Predicting Spans (Joshi et al., 2020)
- [ ] Named Entity Recognition as Dependency Parsing (Yu et al., 2020)
- [ ] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2019)
- [ ] How to Fine-Tune BERT for Text Classification? (Sun et al., 2019)
- [ ] TOE: A Grid-Tagging Discontinuous NER Model Enhanced by Embedding Tag/Word Relations and More Fine-Grained Tags (Liu et al., 2021)

%% kanban:settings
```
{"kanban-plugin":"board","list-collapse":[false,false]}
```
%%